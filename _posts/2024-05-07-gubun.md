---
layout: single
title: "머신러닝(Machine Learning) / 딥러닝(Deep Learning) 지식"
categories : AI
tag: [ML, DL]
---

머신러닝(Machine Learning) / 딥러닝(Deep Learning)에서 알아야 할 지식들

## 참조
tensorflow playground
- [https://playground.tensorflow.org/](https://playground.tensorflow.org/){:target="_blank"}

## Supervised Learning
- 정답과 같이 트레이닝
- 정답= labeling, 정답+data = labeled data
- Linear/logistic regression, decision tree, bayesian classification, neural network, hidden markov model(HMM) 등
- 예측(Regression), 분류(Classification)

## Unsupervised Learning
- 정답없이 데이터만 트레이닝
- K-Means clustering, K-Nearest neighbor clustering, PCA, linear discriminant analysis
- 군집화(Clustering) : 고객세분화(segmentation), 장바구니분석

## Reinforcement Learning
- Feedback 과 reward/penalty
- Monte Carlo methods, Q-learning, Deep Q-learning 등
- 최적화(Optimization) : 대부분 게임의 자동화 (바둑, 포커, 체스 등), 로봇제어, 공정최적화

## 머신러닝(Machine Learning)으로 할수 있는 일

|구분 | 종류 |결과값(y)|대표적인 알고리즘|labeled data|용도|
|-|-|-|-|-|-|
|예측<br>(regression) | supervised<br>learning |연속형<br>(키, 온도, 집값...)|Decision Tree<br>Linear Regression<br>Random Forest<br> Neural Network<br>Gradient Boosting Tree|yes|stock price forecasting predictions|
|분류<br>(classification) |supervised<br>learning| 범주형<br>(남/여, 스팸여부, 허가/불허가, 0~9 숫자 맞추기, 혈액 형 etc)| Decision Tree<br>Random Forest<br>Logistic Regression<br>Neural Netork<br>Gradient Boosting Tree<br>Naive Bayes<br>SVM|yes|보험사기 적발<br>사진 분류<br>고객 이탈 여부|
|군집화<br>(clustering)| unsupervised<br>learning|연속형/범주형| K-means<br>PCA<br>SVD<br>Hidden Markov| no| 고객 세분화|

## Learning Rate
- Learning rate를 작게 해주면 답을 찾는데 시간이 많이 걸리고 크게 해주면 답을 못 찾을 경우도 생긴다
- Learning rate를 잘 알아서 적절하게 조절해주는 것 = optimizer

![learning](/assets/posts/learning.jpg)

## optimizer의 종류
![optimiser](/assets/posts/optimiser.jpg)

## Activation Function 종류
- 오차제곱의 합의 평균값 = Mean Square Error(MSE)
- Layer가 많아지면, 단순 MSE 계산은 parameter들의 단순 계산의 반복(Linear Regression)이라 정답을 알 수 없다. ==> 선형에서 비선형으로
- 따라서 activation 함수를 씌워서 각 단계를 단순화 시킨다
- Activation function의 예
  - Tanh
  - ReLU(Rectified Linear Unit) : back propagation 때 vanishing gradient 현상 방지 
  - Sigmoid (답을 0 or 1 일 때)
  - softmax (답이 여러 개 중 하나일 때)

![activation](/assets/posts/activation.jpg)

## Back Propagation Algorithm
- 학습된 출력값과 실제 값과의 차이인 오차를 계산하여 역방향으로 계산(미분 하고 곱하고 더하는 것)
- 반복해서 parameter를 업데이터 하는 방식
- Layer가 깊어질수록 오차의 값이 역방향으로 가다가 사라지는 현상(vanishing gradient) → ReLU 함수로 해결

![propagation](/assets/posts/propagation.jpg)