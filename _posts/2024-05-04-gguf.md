---
layout: single
title: "gguf 파일 만들기 더이상 헤메지 말자."
categories : AI
tag: [huggingface, llama3, gguf, cmake]
---

gguf 파일을 생성하기 위한 가이드

## 참조
huggingface
- [https://huggingface.co](https://huggingface.co){:target="_blank"}

llama.cpp
- [https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp){:target="_blank"}

k-quants
- [https://github.com/ggerganov/llama.cpp/pull/1684](https://github.com/ggerganov/llama.cpp/pull/1684){:target="_blank"}

Tutorial: How to convert HuggingFace model to GGUF format #2948
- [https://github.com/ggerganov/llama.cpp/discussions/2948](https://github.com/ggerganov/llama.cpp/discussions/2948){:target="_blank"}

Refactor convert.py and add support for Metas official Llama 3 model #6819
- [https://github.com/ggerganov/llama.cpp/issues/6819](https://github.com/ggerganov/llama.cpp/issues/6819){:target="_blank"}

## llama.cpp 준비 (make)

Output Folder : llama.cpp/

```bash
snoopy_kr@MacBookPro AI % git clone https://github.com/ggerganov/llama.cpp
Cloning into 'llama.cpp'...
remote: Enumerating objects: 23768, done.
remote: Counting objects: 100% (127/127), done.
remote: Compressing objects: 100% (87/87), done.
remote: Total 23768 (delta 59), reused 90 (delta 40), pack-reused 23641
Receiving objects: 100% (23768/23768), 38.14 MiB | 21.42 MiB/s, done.
Resolving deltas: 100% (16793/16793), done.
```

```bash
snoopy_kr@MacBookPro AI % cd llama.cpp
```

```bash
snoopy_kr@MacBookPro llama.cpp % make                 
I ccache not found. Consider installing it for faster compilation.
I llama.cpp build info: 
I UNAME_S:   Darwin
I UNAME_P:   i386
I UNAME_M:   x86_64
I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion 
I CXXFLAGS:  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL 
I NVCCFLAGS: -std=c++11 -O3 
I LDFLAGS:   -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 
I CC:        Apple clang version 15.0.0 (clang-1500.1.0.2.5)
I CXX:       Apple clang version 15.0.0 (clang-1500.1.0.2.5)

c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c common/grammar-parser.cpp -o grammar-parser.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c common/build-info.cpp -o build-info.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c common/json-schema-to-grammar.cpp -o json-schema-to-grammar.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c common/console.cpp -o console.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c sgemm.cpp -o sgemm.o
cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion  -c ggml-metal.m -o ggml-metal.o
cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion    -c ggml-alloc.c -o ggml-alloc.o
cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion    -c ggml-backend.c -o ggml-backend.o
cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion     -c ggml-quants.c -o ggml-quants.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c unicode.cpp -o unicode.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c unicode-data.cpp -o unicode-data.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/main/main.cpp -o examples/main/main.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o console.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/main/main.o -o main -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 

====  Run ./main -h for help.  ====

c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/quantize/quantize.cpp -o examples/quantize/quantize.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/quantize/quantize.o -o quantize -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/quantize-stats/quantize-stats.cpp -o examples/quantize-stats/quantize-stats.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  build-info.o ggml.o llama.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/quantize-stats/quantize-stats.o -o quantize-stats -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/perplexity/perplexity.cpp -o examples/perplexity/perplexity.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/perplexity/perplexity.o -o perplexity -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/imatrix/imatrix.cpp -o examples/imatrix/imatrix.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/imatrix/imatrix.o -o imatrix -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/embedding/embedding.cpp -o examples/embedding/embedding.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/embedding/embedding.o -o embedding -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c pocs/vdot/vdot.cpp -o pocs/vdot/vdot.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  ggml.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o pocs/vdot/vdot.o -o vdot -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c pocs/vdot/q8dot.cpp -o pocs/vdot/q8dot.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  ggml.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o pocs/vdot/q8dot.o -o q8dot -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c common/train.cpp -o train.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/train-text-from-scratch/train-text-from-scratch.cpp -o examples/train-text-from-scratch/train-text-from-scratch.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o train.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/train-text-from-scratch/train-text-from-scratch.o -o train-text-from-scratch -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp -o examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  ggml.o llama.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.o -o convert-llama2c-to-ggml -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/simple/simple.cpp -o examples/simple/simple.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/simple/simple.o -o simple -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/batched/batched.cpp -o examples/batched/batched.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/batched/batched.o -o batched -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/batched-bench/batched-bench.cpp -o examples/batched-bench/batched-bench.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  build-info.o ggml.o llama.o common.o sampling.o grammar-parser.o json-schema-to-grammar.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/batched-bench/batched-bench.o -o batched-bench -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/save-load-state/save-load-state.cpp -o examples/save-load-state/save-load-state.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/save-load-state/save-load-state.o -o save-load-state -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/server/server.cpp -o examples/server/server.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o -Iexamples/server examples/server/server.o -o server -framework Accelerate -framework Foundation -framework Metal -framework MetalKit  
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/gguf/gguf.cpp -o examples/gguf/gguf.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  ggml.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/gguf/gguf.o -o gguf -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/gguf-split/gguf-split.cpp -o examples/gguf-split/gguf-split.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/gguf-split/gguf-split.o -o gguf-split -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/eval-callback/eval-callback.cpp -o examples/eval-callback/eval-callback.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/eval-callback/eval-callback.o -o eval-callback -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/llama-bench/llama-bench.cpp -o examples/llama-bench/llama-bench.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/llama-bench/llama-bench.o -o llama-bench -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -static -fPIC -c examples/llava/llava.cpp -o libllava.a -Wno-cast-qual
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/llava/llava-cli.cpp -o examples/llava/llava-cli.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/llava/clip.cpp  -o examples/llava/clip.o -Wno-cast-qual
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/llava/llava.cpp -o examples/llava/llava.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/llava/llava-cli.o examples/llava/clip.o examples/llava/llava.o -o llava-cli -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/baby-llama/baby-llama.cpp -o examples/baby-llama/baby-llama.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o train.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/baby-llama/baby-llama.o -o baby-llama -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/beam-search/beam-search.cpp -o examples/beam-search/beam-search.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/beam-search/beam-search.o -o beam-search -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/retrieval/retrieval.cpp -o examples/retrieval/retrieval.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/retrieval/retrieval.o -o retrieval -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/speculative/speculative.cpp -o examples/speculative/speculative.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/speculative/speculative.o -o speculative -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/infill/infill.cpp -o examples/infill/infill.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o console.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/infill/infill.o -o infill -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/tokenize/tokenize.cpp -o examples/tokenize/tokenize.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/tokenize/tokenize.o -o tokenize -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/benchmark/benchmark-matmult.cpp -o examples/benchmark/benchmark-matmult.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  build-info.o ggml.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/benchmark/benchmark-matmult.o -o benchmark-matmult -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/parallel/parallel.cpp -o examples/parallel/parallel.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/parallel/parallel.o -o parallel -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/finetune/finetune.cpp -o examples/finetune/finetune.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o train.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/finetune/finetune.o -o finetune -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/export-lora/export-lora.cpp -o examples/export-lora/export-lora.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  ggml.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/export-lora/export-lora.o -o export-lora -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/lookahead/lookahead.cpp -o examples/lookahead/lookahead.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/lookahead/lookahead.o -o lookahead -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c common/ngram-cache.cpp -o ngram-cache.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/lookup/lookup.cpp -o examples/lookup/lookup.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  ggml.o llama.o ngram-cache.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/lookup/lookup.o -o lookup -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/lookup/lookup-create.cpp -o examples/lookup/lookup-create.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  ggml.o llama.o ngram-cache.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/lookup/lookup-create.o -o lookup-create -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/lookup/lookup-merge.cpp -o examples/lookup/lookup-merge.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  ggml.o llama.o ngram-cache.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/lookup/lookup-merge.o -o lookup-merge -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/lookup/lookup-stats.cpp -o examples/lookup/lookup-stats.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  ggml.o llama.o ngram-cache.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/lookup/lookup-stats.o -o lookup-stats -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/passkey/passkey.cpp -o examples/passkey/passkey.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/passkey/passkey.o -o passkey -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -c examples/gritlm/gritlm.cpp -o examples/gritlm/gritlm.o
c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wmissing-prototypes -Wextra-semi -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o json-schema-to-grammar.o sgemm.o ggml-metal.o ggml-alloc.o ggml-backend.o ggml-quants.o unicode.o unicode-data.o examples/gritlm/gritlm.o -o gritlm -framework Accelerate -framework Foundation -framework Metal -framework MetalKit 
cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_DARWIN_C_SOURCE -DNDEBUG -DGGML_USE_ACCELERATE -DACCELERATE_NEW_LAPACK -DACCELERATE_LAPACK_ILP64 -DGGML_USE_LLAMAFILE -DGGML_USE_METAL  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wunreachable-code-break -Wunreachable-code-return -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o
```

[ext] huggingface_hub 설치

[ext] download.py 작업

[ext] download model

[ext] llama.cpp requirements 설치

## Model을 gguf로 변경
```bash
snoopy_kr@MacBookPro llama.cpp % python convert.py ../models/Llama-3-Open-Ko-8B --outfile ../models/Llama-3-Open-Ko-8B.gguf --vocab-type bpe
INFO:convert:Loading model file ../models/Llama-3-Open-Ko-8B/model-00001-of-00006.safetensors
INFO:convert:Loading model file ../models/Llama-3-Open-Ko-8B/model-00001-of-00006.safetensors
INFO:convert:Loading model file ../models/Llama-3-Open-Ko-8B/model-00002-of-00006.safetensors
INFO:convert:Loading model file ../models/Llama-3-Open-Ko-8B/model-00003-of-00006.safetensors
INFO:convert:Loading model file ../models/Llama-3-Open-Ko-8B/model-00004-of-00006.safetensors
INFO:convert:Loading model file ../models/Llama-3-Open-Ko-8B/model-00005-of-00006.safetensors
INFO:convert:Loading model file ../models/Llama-3-Open-Ko-8B/model-00006-of-00006.safetensors
INFO:convert:params = Params(n_vocab=128256, n_embd=4096, n_layer=32, n_ctx=8192, n_ff=14336, n_head=32, n_head_kv=8, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=500000.0, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=None, path_model=PosixPath('../models/Llama-3-Open-Ko-8B'))
INFO:convert:Loaded vocab file PosixPath('../models/Llama-3-Open-Ko-8B/tokenizer.json'), type 'bpe'
INFO:convert:Vocab info: <BpeVocab with 128000 base tokens and 256 added tokens>
INFO:convert:Special vocab info: <SpecialVocab with 280147 merges, special tokens {'bos': 128000, 'eos': 128001}, add special tokens unset>
INFO:convert:Writing ../models/Llama-3-Open-Ko-8B.gguf, format 0
WARNING:convert:Ignoring added_tokens.json since model matches vocab size without it.
INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only
INFO:gguf.vocab:Adding 280147 merge(s).
INFO:gguf.vocab:Setting special token type bos to 128000
INFO:gguf.vocab:Setting special token type eos to 128001
INFO:convert:[  1/291] Writing tensor token_embd.weight                      | size 128256 x   4096  | type F32  | T+   9
INFO:convert:[  2/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+  10
INFO:convert:[  3/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  14336  | type F32  | T+  10
INFO:convert:[  4/291] Writing tensor blk.0.ffn_gate.weight                  | size  14336 x   4096  | type F32  | T+  11
INFO:convert:[  5/291] Writing tensor blk.0.ffn_up.weight                    | size  14336 x   4096  | type F32  | T+  11
INFO:convert:[  6/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+  11
INFO:convert:[  7/291] Writing tensor blk.0.attn_k.weight                    | size   1024 x   4096  | type F32  | T+  11
INFO:convert:[  8/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type F32  | T+  11
INFO:convert:[  9/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type F32  | T+  11
INFO:convert:[ 10/291] Writing tensor blk.0.attn_v.weight                    | size   1024 x   4096  | type F32  | T+  11
INFO:convert:[ 11/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+  11
INFO:convert:[ 12/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  14336  | type F32  | T+  12
INFO:convert:[ 13/291] Writing tensor blk.1.ffn_gate.weight                  | size  14336 x   4096  | type F32  | T+  12
INFO:convert:[ 14/291] Writing tensor blk.1.ffn_up.weight                    | size  14336 x   4096  | type F32  | T+  12
INFO:convert:[ 15/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+  12
INFO:convert:[ 16/291] Writing tensor blk.1.attn_k.weight                    | size   1024 x   4096  | type F32  | T+  12
INFO:convert:[ 17/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type F32  | T+  12
INFO:convert:[ 18/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type F32  | T+  12
INFO:convert:[ 19/291] Writing tensor blk.1.attn_v.weight                    | size   1024 x   4096  | type F32  | T+  12
INFO:convert:[ 20/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+  12
INFO:convert:[ 21/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  14336  | type F32  | T+  13
INFO:convert:[ 22/291] Writing tensor blk.2.ffn_gate.weight                  | size  14336 x   4096  | type F32  | T+  13
INFO:convert:[ 23/291] Writing tensor blk.2.ffn_up.weight                    | size  14336 x   4096  | type F32  | T+  14
INFO:convert:[ 24/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+  14
INFO:convert:[ 25/291] Writing tensor blk.2.attn_k.weight                    | size   1024 x   4096  | type F32  | T+  14
INFO:convert:[ 26/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type F32  | T+  14
INFO:convert:[ 27/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type F32  | T+  14
INFO:convert:[ 28/291] Writing tensor blk.2.attn_v.weight                    | size   1024 x   4096  | type F32  | T+  14
INFO:convert:[ 29/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+  14
INFO:convert:[ 30/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  14336  | type F32  | T+  15
INFO:convert:[ 31/291] Writing tensor blk.3.ffn_gate.weight                  | size  14336 x   4096  | type F32  | T+  15
INFO:convert:[ 32/291] Writing tensor blk.3.ffn_up.weight                    | size  14336 x   4096  | type F32  | T+  15
INFO:convert:[ 33/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+  15
INFO:convert:[ 34/291] Writing tensor blk.3.attn_k.weight                    | size   1024 x   4096  | type F32  | T+  15
INFO:convert:[ 35/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type F32  | T+  15
INFO:convert:[ 36/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type F32  | T+  15
INFO:convert:[ 37/291] Writing tensor blk.3.attn_v.weight                    | size   1024 x   4096  | type F32  | T+  15
INFO:convert:[ 38/291] Writing tensor blk.4.ffn_gate.weight                  | size  14336 x   4096  | type F32  | T+  16
INFO:convert:[ 39/291] Writing tensor blk.4.attn_k.weight                    | size   1024 x   4096  | type F32  | T+  16
INFO:convert:[ 40/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type F32  | T+  16
INFO:convert:[ 41/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type F32  | T+  16
INFO:convert:[ 42/291] Writing tensor blk.4.attn_v.weight                    | size   1024 x   4096  | type F32  | T+  16
INFO:convert:[ 43/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+  17
INFO:convert:[ 44/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  17
INFO:convert:[ 45/291] Writing tensor blk.10.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  17
INFO:convert:[ 46/291] Writing tensor blk.10.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  17
INFO:convert:[ 47/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+  18
INFO:convert:[ 48/291] Writing tensor blk.10.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  18
INFO:convert:[ 49/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type F32  | T+  18
INFO:convert:[ 50/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  18
INFO:convert:[ 51/291] Writing tensor blk.10.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  18
INFO:convert:[ 52/291] Writing tensor blk.11.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  18
INFO:convert:[ 53/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type F32  | T+  18
INFO:convert:[ 54/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  18
INFO:convert:[ 55/291] Writing tensor blk.11.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  18
INFO:convert:[ 56/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+  18
INFO:convert:[ 57/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  14336  | type F32  | T+  19
INFO:convert:[ 58/291] Writing tensor blk.4.ffn_up.weight                    | size  14336 x   4096  | type F32  | T+  19
INFO:convert:[ 59/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+  20
INFO:convert:[ 60/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+  20
INFO:convert:[ 61/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  14336  | type F32  | T+  20
INFO:convert:[ 62/291] Writing tensor blk.5.ffn_gate.weight                  | size  14336 x   4096  | type F32  | T+  20
INFO:convert:[ 63/291] Writing tensor blk.5.ffn_up.weight                    | size  14336 x   4096  | type F32  | T+  20
INFO:convert:[ 64/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+  20
INFO:convert:[ 65/291] Writing tensor blk.5.attn_k.weight                    | size   1024 x   4096  | type F32  | T+  20
INFO:convert:[ 66/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type F32  | T+  20
INFO:convert:[ 67/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type F32  | T+  20
INFO:convert:[ 68/291] Writing tensor blk.5.attn_v.weight                    | size   1024 x   4096  | type F32  | T+  20
INFO:convert:[ 69/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+  20
INFO:convert:[ 70/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  14336  | type F32  | T+  21
INFO:convert:[ 71/291] Writing tensor blk.6.ffn_gate.weight                  | size  14336 x   4096  | type F32  | T+  22
INFO:convert:[ 72/291] Writing tensor blk.6.ffn_up.weight                    | size  14336 x   4096  | type F32  | T+  22
INFO:convert:[ 73/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+  22
INFO:convert:[ 74/291] Writing tensor blk.6.attn_k.weight                    | size   1024 x   4096  | type F32  | T+  22
INFO:convert:[ 75/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type F32  | T+  22
INFO:convert:[ 76/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type F32  | T+  22
INFO:convert:[ 77/291] Writing tensor blk.6.attn_v.weight                    | size   1024 x   4096  | type F32  | T+  22
INFO:convert:[ 78/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+  22
INFO:convert:[ 79/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  14336  | type F32  | T+  23
INFO:convert:[ 80/291] Writing tensor blk.7.ffn_gate.weight                  | size  14336 x   4096  | type F32  | T+  23
INFO:convert:[ 81/291] Writing tensor blk.7.ffn_up.weight                    | size  14336 x   4096  | type F32  | T+  23
INFO:convert:[ 82/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+  24
INFO:convert:[ 83/291] Writing tensor blk.7.attn_k.weight                    | size   1024 x   4096  | type F32  | T+  24
INFO:convert:[ 84/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type F32  | T+  24
INFO:convert:[ 85/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type F32  | T+  24
INFO:convert:[ 86/291] Writing tensor blk.7.attn_v.weight                    | size   1024 x   4096  | type F32  | T+  24
INFO:convert:[ 87/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+  24
INFO:convert:[ 88/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  14336  | type F32  | T+  25
INFO:convert:[ 89/291] Writing tensor blk.8.ffn_gate.weight                  | size  14336 x   4096  | type F32  | T+  26
INFO:convert:[ 90/291] Writing tensor blk.8.ffn_up.weight                    | size  14336 x   4096  | type F32  | T+  26
INFO:convert:[ 91/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+  26
INFO:convert:[ 92/291] Writing tensor blk.8.attn_k.weight                    | size   1024 x   4096  | type F32  | T+  26
INFO:convert:[ 93/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type F32  | T+  26
INFO:convert:[ 94/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type F32  | T+  26
INFO:convert:[ 95/291] Writing tensor blk.8.attn_v.weight                    | size   1024 x   4096  | type F32  | T+  26
INFO:convert:[ 96/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+  26
INFO:convert:[ 97/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  14336  | type F32  | T+  27
INFO:convert:[ 98/291] Writing tensor blk.9.ffn_gate.weight                  | size  14336 x   4096  | type F32  | T+  27
INFO:convert:[ 99/291] Writing tensor blk.9.ffn_up.weight                    | size  14336 x   4096  | type F32  | T+  28
INFO:convert:[100/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+  28
INFO:convert:[101/291] Writing tensor blk.9.attn_k.weight                    | size   1024 x   4096  | type F32  | T+  28
INFO:convert:[102/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type F32  | T+  28
INFO:convert:[103/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type F32  | T+  28
INFO:convert:[104/291] Writing tensor blk.9.attn_v.weight                    | size   1024 x   4096  | type F32  | T+  28
INFO:convert:[105/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+  28
INFO:convert:[106/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  30
INFO:convert:[107/291] Writing tensor blk.11.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  31
INFO:convert:[108/291] Writing tensor blk.11.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  31
INFO:convert:[109/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+  31
INFO:convert:[110/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+  31
INFO:convert:[111/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  31
INFO:convert:[112/291] Writing tensor blk.12.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  32
INFO:convert:[113/291] Writing tensor blk.12.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  32
INFO:convert:[114/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+  32
INFO:convert:[115/291] Writing tensor blk.12.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  32
INFO:convert:[116/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type F32  | T+  32
INFO:convert:[117/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  32
INFO:convert:[118/291] Writing tensor blk.12.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  32
INFO:convert:[119/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+  32
INFO:convert:[120/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  33
INFO:convert:[121/291] Writing tensor blk.13.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  33
INFO:convert:[122/291] Writing tensor blk.13.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  33
INFO:convert:[123/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+  34
INFO:convert:[124/291] Writing tensor blk.13.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  34
INFO:convert:[125/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type F32  | T+  34
INFO:convert:[126/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  34
INFO:convert:[127/291] Writing tensor blk.13.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  34
INFO:convert:[128/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+  34
INFO:convert:[129/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  35
INFO:convert:[130/291] Writing tensor blk.14.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  35
INFO:convert:[131/291] Writing tensor blk.14.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  35
INFO:convert:[132/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+  35
INFO:convert:[133/291] Writing tensor blk.14.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  35
INFO:convert:[134/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type F32  | T+  35
INFO:convert:[135/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  36
INFO:convert:[136/291] Writing tensor blk.14.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  36
INFO:convert:[137/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+  36
INFO:convert:[138/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  38
INFO:convert:[139/291] Writing tensor blk.15.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  38
INFO:convert:[140/291] Writing tensor blk.15.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  38
INFO:convert:[141/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+  38
INFO:convert:[142/291] Writing tensor blk.15.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  38
INFO:convert:[143/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type F32  | T+  38
INFO:convert:[144/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  38
INFO:convert:[145/291] Writing tensor blk.15.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  38
INFO:convert:[146/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+  38
INFO:convert:[147/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  39
INFO:convert:[148/291] Writing tensor blk.16.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  39
INFO:convert:[149/291] Writing tensor blk.16.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  39
INFO:convert:[150/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+  40
INFO:convert:[151/291] Writing tensor blk.16.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  40
INFO:convert:[152/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type F32  | T+  40
INFO:convert:[153/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  40
INFO:convert:[154/291] Writing tensor blk.16.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  40
INFO:convert:[155/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+  40
INFO:convert:[156/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  41
INFO:convert:[157/291] Writing tensor blk.17.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  41
INFO:convert:[158/291] Writing tensor blk.17.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  41
INFO:convert:[159/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+  41
INFO:convert:[160/291] Writing tensor blk.17.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  41
INFO:convert:[161/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type F32  | T+  41
INFO:convert:[162/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  41
INFO:convert:[163/291] Writing tensor blk.17.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  41
INFO:convert:[164/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+  41
INFO:convert:[165/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  43
INFO:convert:[166/291] Writing tensor blk.18.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  43
INFO:convert:[167/291] Writing tensor blk.18.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  43
INFO:convert:[168/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+  44
INFO:convert:[169/291] Writing tensor blk.18.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  44
INFO:convert:[170/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type F32  | T+  44
INFO:convert:[171/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  44
INFO:convert:[172/291] Writing tensor blk.18.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  44
INFO:convert:[173/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+  44
INFO:convert:[174/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  45
INFO:convert:[175/291] Writing tensor blk.19.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  46
INFO:convert:[176/291] Writing tensor blk.19.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  46
INFO:convert:[177/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+  46
INFO:convert:[178/291] Writing tensor blk.19.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  46
INFO:convert:[179/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type F32  | T+  46
INFO:convert:[180/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  46
INFO:convert:[181/291] Writing tensor blk.19.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  46
INFO:convert:[182/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+  46
INFO:convert:[183/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  47
INFO:convert:[184/291] Writing tensor blk.20.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  48
INFO:convert:[185/291] Writing tensor blk.20.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  48
INFO:convert:[186/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+  48
INFO:convert:[187/291] Writing tensor blk.20.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  48
INFO:convert:[188/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type F32  | T+  48
INFO:convert:[189/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  48
INFO:convert:[190/291] Writing tensor blk.20.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  48
INFO:convert:[191/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+  48
INFO:convert:[192/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  49
INFO:convert:[193/291] Writing tensor blk.21.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  49
INFO:convert:[194/291] Writing tensor blk.21.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  50
INFO:convert:[195/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+  50
INFO:convert:[196/291] Writing tensor blk.21.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  50
INFO:convert:[197/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type F32  | T+  50
INFO:convert:[198/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  50
INFO:convert:[199/291] Writing tensor blk.21.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  50
INFO:convert:[200/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+  50
INFO:convert:[201/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  50
INFO:convert:[202/291] Writing tensor blk.22.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  52
INFO:convert:[203/291] Writing tensor blk.22.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  52
INFO:convert:[204/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+  52
INFO:convert:[205/291] Writing tensor blk.22.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  52
INFO:convert:[206/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type F32  | T+  52
INFO:convert:[207/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  52
INFO:convert:[208/291] Writing tensor blk.22.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  52
INFO:convert:[209/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+  52
INFO:convert:[210/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  53
INFO:convert:[211/291] Writing tensor blk.23.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  54
INFO:convert:[212/291] Writing tensor blk.23.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  55
INFO:convert:[213/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+  56
INFO:convert:[214/291] Writing tensor blk.23.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  56
INFO:convert:[215/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type F32  | T+  56
INFO:convert:[216/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  56
INFO:convert:[217/291] Writing tensor blk.23.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  56
INFO:convert:[218/291] Writing tensor blk.24.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  56
INFO:convert:[219/291] Writing tensor blk.24.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  56
INFO:convert:[220/291] Writing tensor blk.24.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  56
INFO:convert:[221/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type F32  | T+  56
INFO:convert:[222/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  56
INFO:convert:[223/291] Writing tensor blk.24.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  56
INFO:convert:[224/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+  56
INFO:convert:[225/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  58
INFO:convert:[226/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+  58
INFO:convert:[227/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+  58
INFO:convert:[228/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  59
INFO:convert:[229/291] Writing tensor blk.25.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  60
INFO:convert:[230/291] Writing tensor blk.25.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  60
INFO:convert:[231/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+  60
INFO:convert:[232/291] Writing tensor blk.25.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  60
INFO:convert:[233/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type F32  | T+  60
INFO:convert:[234/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  60
INFO:convert:[235/291] Writing tensor blk.25.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  60
INFO:convert:[236/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+  60
INFO:convert:[237/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  62
INFO:convert:[238/291] Writing tensor blk.26.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  62
INFO:convert:[239/291] Writing tensor blk.26.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  62
INFO:convert:[240/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+  62
INFO:convert:[241/291] Writing tensor blk.26.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  62
INFO:convert:[242/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type F32  | T+  62
INFO:convert:[243/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  62
INFO:convert:[244/291] Writing tensor blk.26.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  64
INFO:convert:[245/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+  64
INFO:convert:[246/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  64
INFO:convert:[247/291] Writing tensor blk.27.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  64
INFO:convert:[248/291] Writing tensor blk.27.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  64
INFO:convert:[249/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+  65
INFO:convert:[250/291] Writing tensor blk.27.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  65
INFO:convert:[251/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type F32  | T+  65
INFO:convert:[252/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  65
INFO:convert:[253/291] Writing tensor blk.27.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  65
INFO:convert:[254/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+  65
INFO:convert:[255/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  67
INFO:convert:[256/291] Writing tensor blk.28.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  68
INFO:convert:[257/291] Writing tensor blk.28.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  68
INFO:convert:[258/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+  68
INFO:convert:[259/291] Writing tensor blk.28.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  68
INFO:convert:[260/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type F32  | T+  68
INFO:convert:[261/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  68
INFO:convert:[262/291] Writing tensor blk.28.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  68
INFO:convert:[263/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+  68
INFO:convert:[264/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  70
INFO:convert:[265/291] Writing tensor blk.29.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  70
INFO:convert:[266/291] Writing tensor blk.29.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  71
INFO:convert:[267/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+  71
INFO:convert:[268/291] Writing tensor blk.29.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  71
INFO:convert:[269/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type F32  | T+  71
INFO:convert:[270/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  71
INFO:convert:[271/291] Writing tensor blk.29.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  71
INFO:convert:[272/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+  71
INFO:convert:[273/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  72
INFO:convert:[274/291] Writing tensor blk.30.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  72
INFO:convert:[275/291] Writing tensor blk.30.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  73
INFO:convert:[276/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+  73
INFO:convert:[277/291] Writing tensor blk.30.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  73
INFO:convert:[278/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type F32  | T+  73
INFO:convert:[279/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  73
INFO:convert:[280/291] Writing tensor blk.30.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  73
INFO:convert:[281/291] Writing tensor blk.31.ffn_gate.weight                 | size  14336 x   4096  | type F32  | T+  74
INFO:convert:[282/291] Writing tensor blk.31.attn_k.weight                   | size   1024 x   4096  | type F32  | T+  74
INFO:convert:[283/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type F32  | T+  74
INFO:convert:[284/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type F32  | T+  74
INFO:convert:[285/291] Writing tensor blk.31.attn_v.weight                   | size   1024 x   4096  | type F32  | T+  74
INFO:convert:[286/291] Writing tensor output.weight                          | size 128256 x   4096  | type F32  | T+  87
INFO:convert:[287/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+  89
INFO:convert:[288/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  14336  | type F32  | T+  89
INFO:convert:[289/291] Writing tensor blk.31.ffn_up.weight                   | size  14336 x   4096  | type F32  | T+  90
INFO:convert:[290/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+  90
INFO:convert:[291/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+  90
INFO:convert:Wrote ../models/Llama-3-Open-Ko-8B.gguf
```

## gguf 양자화
```bash
snoopy_kr@MacBookPro Models % ../llama.cpp/build/bin/quantize Llama-3-Open-Ko-8B.gguf Llama-3-Open-Ko-8B-q5_k_m.gguf Q5_K_M
main: build = 2784 (a2ac89d6)
main: built with Apple clang version 15.0.0 (clang-1500.1.0.2.5) for x86_64-apple-darwin22.6.0
main: quantizing 'Llama-3-Open-Ko-8B.gguf' to 'Llama-3-Open-Ko-8B-q5_k_m.gguf' as Q5_K_M
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from Llama-3-Open-Ko-8B.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Llama-3-Open-Ko-8B
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336

[ === 생략 === ]

[ 286/ 291]                        output.weight - [ 4096, 128256,     1,     1], type =    f16, converting to q6_K .. size =  1002.00 MiB ->   410.98 MiB
[ 287/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 288/ 291]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB
[ 289/ 291]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 290/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
llama_model_quantize_internal: model size  = 15317.02 MB
llama_model_quantize_internal: quant size  =  5459.93 MB

main: quantize time = 102716.81 ms
main:    total time = 102716.81 ms
```

## [ext] download.py 작업
```python
from huggingface_hub import snapshot_download
model_id="beomi/Llama-3-Open-Ko-8B"
snapshot_download(repo_id=model_id, local_dir="Llama-3-Open-Ko-8B",
                  local_dir_use_symlinks=False, revision="main")
```

## [ext] download model
```bash
snoopy_kr@MacBookPro models % python download.py 
/Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1194: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.
For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.
  warnings.warn(
generation_config.json: 100%|███████████████████| 132/132 [00:00<00:00, 751kB/s]
.gitattributes: 100%|██████████████████████| 1.57k/1.57k [00:00<00:00, 11.1MB/s]
config.json: 100%|█████████████████████████████| 698/698 [00:00<00:00, 4.48MB/s]
model.safetensors.index.json: 100%|████████| 23.9k/23.9k [00:00<00:00, 2.72MB/s]
README.md: 100%|███████████████████████████| 8.21k/8.21k [00:00<00:00, 46.9MB/s]
model-00004-of-00006.safetensors: 100%|████| 2.94G/2.94G [03:44<00:00, 13.1MB/s]
model-00002-of-00006.safetensors: 100%|████| 2.94G/2.94G [04:14<00:00, 11.5MB/s]
model-00006-of-00006.safetensors: 100%|████| 1.29G/1.29G [04:54<00:00, 4.36MB/s]
model-00003-of-00006.safetensors: 100%|████| 2.97G/2.97G [05:06<00:00, 9.67MB/s]
pytorch_model-00002-of-00006.bin: 100%|████| 2.94G/2.94G [06:01<00:00, 8.11MB/s]
pytorch_model.bin.index.json: 100%|████████| 23.9k/23.9k [00:00<00:00, 90.4MB/s]
special_tokens_map.json: 100%|█████████████████| 301/301 [00:00<00:00, 3.85MB/s]
tokenizer.json: 100%|██████████████████████| 9.09M/9.09M [00:03<00:00, 2.45MB/s]
tokenizer_config.json: 100%|███████████████| 51.0k/51.0k [00:00<00:00, 12.9MB/s]
pytorch_model-00006-of-00006.bin: 100%|████| 1.29G/1.29G [01:46<00:00, 12.0MB/s]
pytorch_model-00004-of-00006.bin: 100%|████| 2.94G/2.94G [03:11<00:00, 15.4MB/s]
pytorch_model-00003-of-00006.bin: 100%|████| 2.97G/2.97G [03:45<00:00, 13.2MB/s]
model-00005-of-00006.safetensors: 100%|████| 2.94G/2.94G [08:04<00:00, 6.06MB/s]
pytorch_model-00001-of-00006.bin: 100%|████| 3.00G/3.00G [08:37<00:00, 5.79MB/s]
model-00001-of-00006.safetensors: 100%|████| 3.00G/3.00G [08:46<00:00, 5.69MB/s]
pytorch_model-00005-of-00006.bin: 100%|████| 2.94G/2.94G [03:53<00:00, 12.6MB/s]
Fetching 21 files: 100%|████████████████████████| 21/21 [08:52<00:00, 25.35s/it]
```

## [ext] llama.cpp requirements 설치
```bash
snoopy_kr@MacBookPro llama.cpp % pip install -r requirements.txt
Requirement already satisfied: numpy~=1.24.4 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from -r llama.cpp/./requirements/requirements-convert.txt (line 1)) (1.24.4)
Requirement already satisfied: sentencepiece~=0.1.98 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from -r llama.cpp/./requirements/requirements-convert.txt (line 2)) (0.1.99)
Requirement already satisfied: transformers<5.0.0,>=4.35.2 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from -r llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.40.1)
Requirement already satisfied: gguf>=0.1.0 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from -r llama.cpp/./requirements/requirements-convert.txt (line 4)) (0.6.0)
Requirement already satisfied: protobuf<5.0.0,>=4.21.0 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from -r llama.cpp/./requirements/requirements-convert.txt (line 5)) (4.25.3)
Requirement already satisfied: torch~=2.1.1 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from -r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.2)
Requirement already satisfied: einops~=0.7.0 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from -r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 3)) (0.7.0)
Requirement already satisfied: filelock in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.13.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.23.0)
Requirement already satisfied: packaging>=20.0 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (23.1)
Requirement already satisfied: pyyaml>=5.1 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (6.0.1)
Requirement already satisfied: regex!=2019.12.17 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2023.10.3)
Requirement already satisfied: requests in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.31.0)
Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.19.1)
Requirement already satisfied: safetensors>=0.4.1 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.4.3)
Requirement already satisfied: tqdm>=4.27 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.65.0)
Requirement already satisfied: typing-extensions in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (4.9.0)
Requirement already satisfied: sympy in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.12)
Requirement already satisfied: networkx in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.1)
Requirement already satisfied: jinja2 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.1.3)
Requirement already satisfied: fsspec in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2023.10.0)
Requirement already satisfied: MarkupSafe>=2.0 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from jinja2->torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.3)
Requirement already satisfied: charset-normalizer<4,>=2 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.0.7)
Requirement already satisfied: certifi>=2017.4.17 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2024.2.2)
Requirement already satisfied: mpmath>=0.19 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from sympy->torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.3.0)
```

## [ref] convert.py help
```bash
snoopy_kr@MacBookPro llama.cpp % python convert.py -h         
usage: convert.py [-h] [--dump] [--dump-single] [--vocab-only] [--no-vocab]
                  [--outtype {f32,f16,q8_0}] [--vocab-dir VOCAB_DIR]
                  [--vocab-type VOCAB_TYPE] [--outfile OUTFILE] [--ctx CTX]
                  [--concurrency CONCURRENCY] [--big-endian] [--pad-vocab]
                  [--skip-unknown] [--verbose]
                  model

Convert a LLaMA model to a GGML compatible file

positional arguments:
  model                 directory containing model file, or model file itself
                        (*.pth, *.pt, *.bin)

options:
  -h, --help            show this help message and exit
  --dump                don't convert, just show what's in the model
  --dump-single         don't convert, just show what's in a single model file
  --vocab-only          extract only the vocab
  --no-vocab            store model without the vocab
  --outtype {f32,f16,q8_0}
                        output format - note: q8_0 may be very slow (default:
                        f16 or f32 based on input)
  --vocab-dir VOCAB_DIR
                        directory containing tokenizer.model, if separate from
                        model file
  --vocab-type VOCAB_TYPE
                        vocab types to try in order, choose from 'spm', 'bpe',
                        'hfft' (default: spm,hfft)
  --outfile OUTFILE     path to write to; default: based on input
  --ctx CTX             model training context (default: based on input)
  --concurrency CONCURRENCY
                        concurrency used for conversion (default: 8)
  --big-endian          model is executed on big endian machine
  --pad-vocab           add pad tokens when model vocab expects more than
                        tokenizer metadata provides
  --skip-unknown        skip unknown tensor names instead of failing
  --verbose             increase output verbosity
```  
