---
layout: single
title: "gguf 파일 생성하기 A에서 Z까지"
categories : AI
tag: [huggingface, llama3, gguf, cmake]
toc: true
---

gguf 파일 생성하기 처음과 끝.

## 참조
huggingface
- [https://huggingface.co](https://huggingface.co){:target="_blank"}

llama.cpp
- [https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp){:target="_blank"}

k-양자화 방식
- [https://github.com/ggerganov/llama.cpp/pull/1684](https://github.com/ggerganov/llama.cpp/pull/1684){:target="_blank"}

Tutorial: How to convert HuggingFace model to GGUF format #2948
- [https://github.com/ggerganov/llama.cpp/discussions/2948](https://github.com/ggerganov/llama.cpp/discussions/2948){:target="_blank"}

Refactor convert.py and add support for Metas official Llama 3 model #6819
- [https://github.com/ggerganov/llama.cpp/issues/6819](https://github.com/ggerganov/llama.cpp/issues/6819){:target="_blank"}

## llama.cpp 준비
```bash
snoopy_kr@Leeui-iMac Working % git clone https://github.com/ggerganov/llama.cpp
Cloning into 'llama.cpp'...
remote: Enumerating objects: 23768, done.
remote: Counting objects: 100% (127/127), done.
remote: Compressing objects: 100% (87/87), done.
remote: Total 23768 (delta 59), reused 90 (delta 40), pack-reused 23641
Receiving objects: 100% (23768/23768), 38.14 MiB | 21.42 MiB/s, done.
Resolving deltas: 100% (16793/16793), done.
```

```bash
snoopy_kr@Leeui-iMac Working % cd llama.cpp
```

```bash
snoopy_kr@Leeui-iMac llama.cpp % mkdir build

```

```bash
snoopy_kr@Leeui-iMac llama.cpp % cd build
```

```bash
snoopy_kr@Leeui-iMac build % cmake ..
-- The C compiler identification is AppleClang 15.0.0.15000100
-- The CXX compiler identification is AppleClang 15.0.0.15000100
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/local/bin/git (found version "2.43.0")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- Accelerate framework found
-- Metal framework found
-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with LLAMA_CCACHE=OFF
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- x86 detected
-- Configuring done (5.6s)
-- Generating done (0.4s)
-- Build files have been written to: /Users/snoopy_kr/Working/llama.cpp/build
```

```bash
snoopy_kr@Leeui-iMac build % cmake --build . --config Release
[  0%] Compiling Metal kernels
/Users/snoopy_kr/Working/llama.cpp/build/bin/ggml-metal.metal:2178:17: warning: unused variable 'Q8' [-Wunused-variable]
    const short Q8 = Q/8;
                ^
1 warning generated.
[  0%] Built target ggml-metal
[  0%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  1%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  2%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  2%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml-metal.m.o
[  4%] Building CXX object CMakeFiles/ggml.dir/sgemm.cpp.o
[  4%] Built target ggml
[  5%] Linking CXX static library libggml_static.a
[  5%] Built target ggml_static
[  5%] Building CXX object CMakeFiles/llama.dir/llama.cpp.o
[  6%] Building CXX object CMakeFiles/llama.dir/unicode.cpp.o
[  7%] Building CXX object CMakeFiles/llama.dir/unicode-data.cpp.o
[  7%] Linking CXX static library libllama.a
[  7%] Built target llama
[  8%] Generating build details from Git
-- Found Git: /usr/local/bin/git (found version "2.43.0")
[  9%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  9%] Built target build_info
[  9%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 10%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 11%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 11%] Building CXX object common/CMakeFiles/common.dir/grammar-parser.cpp.o
[ 12%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 13%] Building CXX object common/CMakeFiles/common.dir/train.cpp.o
[ 13%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 14%] Linking CXX static library libcommon.a
[ 14%] Built target common
[ 14%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 15%] Linking CXX executable ../bin/test-tokenizer-0
[ 15%] Built target test-tokenizer-0
[ 16%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 16%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 16%] Built target test-tokenizer-1-bpe
[ 17%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 18%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 18%] Built target test-tokenizer-1-spm
[ 18%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 19%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 20%] Linking CXX executable ../bin/test-quantize-fns
[ 20%] Built target test-quantize-fns
[ 20%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 21%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 22%] Linking CXX executable ../bin/test-quantize-perf
[ 22%] Built target test-quantize-perf
[ 22%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 23%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 24%] Linking CXX executable ../bin/test-sampling
[ 24%] Built target test-sampling
[ 24%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 25%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 26%] Linking CXX executable ../bin/test-chat-template
[ 26%] Built target test-chat-template
[ 26%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 27%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 28%] Linking CXX executable ../bin/test-grammar-parser
[ 28%] Built target test-grammar-parser
[ 28%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 29%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 30%] Linking CXX executable ../bin/test-llama-grammar
[ 30%] Built target test-llama-grammar
[ 30%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 32%] Linking CXX executable ../bin/test-grammar-integration
[ 32%] Built target test-grammar-integration
[ 32%] Building CXX object tests/CMakeFiles/test-grad0.dir/test-grad0.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-grad0.dir/get-model.cpp.o
[ 34%] Linking CXX executable ../bin/test-grad0
[ 34%] Built target test-grad0
[ 35%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 36%] Linking CXX executable ../bin/test-backend-ops
[ 36%] Built target test-backend-ops
[ 36%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 38%] Linking CXX executable ../bin/test-rope
[ 38%] Built target test-rope
[ 38%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 40%] Linking CXX executable ../bin/test-model-load-cancel
[ 40%] Built target test-model-load-cancel
[ 41%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-autorelease
[ 42%] Built target test-autorelease
[ 42%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 44%] Built target test-json-schema-to-grammar
[ 45%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 46%] Linking CXX executable ../bin/test-c
[ 46%] Built target test-c
[ 46%] Building CXX object examples/baby-llama/CMakeFiles/baby-llama.dir/baby-llama.cpp.o
[ 47%] Linking CXX executable ../../bin/baby-llama
[ 47%] Built target baby-llama
[ 48%] Building CXX object examples/batched/CMakeFiles/batched.dir/batched.cpp.o
[ 48%] Linking CXX executable ../../bin/batched
[ 48%] Built target batched
[ 49%] Building CXX object examples/batched-bench/CMakeFiles/batched-bench.dir/batched-bench.cpp.o
[ 50%] Linking CXX executable ../../bin/batched-bench
[ 50%] Built target batched-bench
[ 50%] Building CXX object examples/beam-search/CMakeFiles/beam-search.dir/beam-search.cpp.o
[ 51%] Linking CXX executable ../../bin/beam-search
[ 51%] Built target beam-search
[ 52%] Building CXX object examples/benchmark/CMakeFiles/benchmark.dir/benchmark-matmult.cpp.o
[ 52%] Linking CXX executable ../../bin/benchmark
[ 52%] Built target benchmark
[ 53%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 53%] Linking CXX executable ../../bin/convert-llama2c-to-ggml
[ 53%] Built target convert-llama2c-to-ggml
[ 54%] Building CXX object examples/embedding/CMakeFiles/embedding.dir/embedding.cpp.o
[ 55%] Linking CXX executable ../../bin/embedding
[ 55%] Built target embedding
[ 55%] Building CXX object examples/eval-callback/CMakeFiles/eval-callback.dir/eval-callback.cpp.o
[ 56%] Linking CXX executable ../../bin/eval-callback
[ 56%] Built target eval-callback
[ 57%] Building CXX object examples/finetune/CMakeFiles/finetune.dir/finetune.cpp.o
[ 58%] Linking CXX executable ../../bin/finetune
[ 58%] Built target finetune
[ 58%] Building CXX object examples/gritlm/CMakeFiles/gritlm.dir/gritlm.cpp.o
[ 59%] Linking CXX executable ../../bin/gritlm
[ 59%] Built target gritlm
[ 60%] Building CXX object examples/gguf-split/CMakeFiles/gguf-split.dir/gguf-split.cpp.o
[ 61%] Linking CXX executable ../../bin/gguf-split
[ 61%] Built target gguf-split
[ 62%] Building CXX object examples/infill/CMakeFiles/infill.dir/infill.cpp.o
[ 63%] Linking CXX executable ../../bin/infill
[ 63%] Built target infill
[ 64%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 65%] Linking CXX executable ../../bin/llama-bench
[ 65%] Built target llama-bench
[ 65%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 66%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 66%] Built target llava
[ 67%] Linking CXX static library libllava_static.a
[ 67%] Built target llava_static
[ 68%] Building CXX object examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o
[ 68%] Linking CXX executable ../../bin/llava-cli
[ 68%] Built target llava-cli
[ 68%] Building CXX object examples/main/CMakeFiles/main.dir/main.cpp.o
[ 69%] Linking CXX executable ../../bin/main
[ 69%] Built target main
[ 69%] Building CXX object examples/tokenize/CMakeFiles/tokenize.dir/tokenize.cpp.o
[ 70%] Linking CXX executable ../../bin/tokenize
[ 70%] Built target tokenize
[ 71%] Building CXX object examples/parallel/CMakeFiles/parallel.dir/parallel.cpp.o
[ 71%] Linking CXX executable ../../bin/parallel
[ 71%] Built target parallel
[ 71%] Building CXX object examples/perplexity/CMakeFiles/perplexity.dir/perplexity.cpp.o
[ 72%] Linking CXX executable ../../bin/perplexity
[ 72%] Built target perplexity
[ 73%] Building CXX object examples/quantize/CMakeFiles/quantize.dir/quantize.cpp.o
[ 74%] Linking CXX executable ../../bin/quantize
ld: warning: ignoring duplicate libraries: '../../libllama.a'
[ 74%] Built target quantize
[ 74%] Building CXX object examples/quantize-stats/CMakeFiles/quantize-stats.dir/quantize-stats.cpp.o
[ 75%] Linking CXX executable ../../bin/quantize-stats
[ 75%] Built target quantize-stats
[ 76%] Building CXX object examples/retrieval/CMakeFiles/retrieval.dir/retrieval.cpp.o
[ 76%] Linking CXX executable ../../bin/retrieval
[ 76%] Built target retrieval
[ 77%] Building CXX object examples/save-load-state/CMakeFiles/save-load-state.dir/save-load-state.cpp.o
[ 78%] Linking CXX executable ../../bin/save-load-state
[ 78%] Built target save-load-state
[ 78%] Building CXX object examples/simple/CMakeFiles/simple.dir/simple.cpp.o
[ 79%] Linking CXX executable ../../bin/simple
[ 79%] Built target simple
[ 80%] Building CXX object examples/passkey/CMakeFiles/passkey.dir/passkey.cpp.o
[ 81%] Linking CXX executable ../../bin/passkey
[ 81%] Built target passkey
[ 82%] Building CXX object examples/speculative/CMakeFiles/speculative.dir/speculative.cpp.o
[ 82%] Linking CXX executable ../../bin/speculative
[ 82%] Built target speculative
[ 83%] Building CXX object examples/lookahead/CMakeFiles/lookahead.dir/lookahead.cpp.o
[ 83%] Linking CXX executable ../../bin/lookahead
[ 83%] Built target lookahead
[ 84%] Building CXX object examples/lookup/CMakeFiles/lookup.dir/lookup.cpp.o
[ 85%] Linking CXX executable ../../bin/lookup
[ 85%] Built target lookup
[ 85%] Building CXX object examples/lookup/CMakeFiles/lookup-create.dir/lookup-create.cpp.o
[ 86%] Linking CXX executable ../../bin/lookup-create
[ 86%] Built target lookup-create
[ 87%] Building CXX object examples/lookup/CMakeFiles/lookup-merge.dir/lookup-merge.cpp.o
[ 87%] Linking CXX executable ../../bin/lookup-merge
[ 87%] Built target lookup-merge
[ 88%] Building CXX object examples/lookup/CMakeFiles/lookup-stats.dir/lookup-stats.cpp.o
[ 89%] Linking CXX executable ../../bin/lookup-stats
[ 89%] Built target lookup-stats
[ 90%] Building CXX object examples/gguf/CMakeFiles/gguf.dir/gguf.cpp.o
[ 90%] Linking CXX executable ../../bin/gguf
[ 90%] Built target gguf
[ 91%] Building CXX object examples/train-text-from-scratch/CMakeFiles/train-text-from-scratch.dir/train-text-from-scratch.cpp.o
[ 91%] Linking CXX executable ../../bin/train-text-from-scratch
[ 91%] Built target train-text-from-scratch
[ 92%] Building CXX object examples/imatrix/CMakeFiles/imatrix.dir/imatrix.cpp.o
[ 92%] Linking CXX executable ../../bin/imatrix
[ 92%] Built target imatrix
[ 92%] Generating json-schema-to-grammar.mjs.hpp
[ 93%] Generating completion.js.hpp
[ 93%] Generating index.html.hpp
[ 94%] Generating index.js.hpp
[ 95%] Building CXX object examples/server/CMakeFiles/server.dir/server.cpp.o
[ 96%] Linking CXX executable ../../bin/server
[ 96%] Built target server
[ 97%] Building CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o
[ 97%] Linking CXX executable ../../bin/export-lora
[ 97%] Built target export-lora
[ 98%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 99%] Linking CXX executable ../../bin/vdot
[ 99%] Built target vdot
[100%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[100%] Linking CXX executable ../../bin/q8dot
[100%] Built target q8dot
```

## Model Convert to gguf format
```bash
snoopy_kr@Leeui-iMac Models % python ../llama.cpp/convert-hf-to-gguf.py Llama-3-Open-Ko-8B --outfile Llama-3-Open-Ko-8B.gguf
INFO:hf-to-gguf:Loading model: Llama-3-Open-Ko-8B
INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only
INFO:hf-to-gguf:Set model parameters
INFO:hf-to-gguf:gguf: context length = 8192
INFO:hf-to-gguf:gguf: embedding length = 4096
INFO:hf-to-gguf:gguf: feed forward length = 14336
INFO:hf-to-gguf:gguf: head count = 32
INFO:hf-to-gguf:gguf: key-value head count = 8
INFO:hf-to-gguf:gguf: rope theta = 500000.0
INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05
INFO:hf-to-gguf:gguf: file type = 1
INFO:hf-to-gguf:Set model tokenizer
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
INFO:gguf.vocab:Adding 280147 merge(s).
INFO:gguf.vocab:Setting special token type bos to 128000
INFO:gguf.vocab:Setting special token type eos to 128001
INFO:gguf.vocab:Setting chat_template to {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>

'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>

' }}{% endif %}
INFO:hf-to-gguf:Exporting model to 'Llama-3-Open-Ko-8B.gguf'
INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00006.safetensors'
INFO:hf-to-gguf:token_embd.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.0.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.0.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.0.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.0.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.0.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.0.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.0.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.0.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.0.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.1.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.1.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.1.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.1.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.1.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.1.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.1.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.1.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.1.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.2.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.2.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.2.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.2.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.2.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.2.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.2.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.2.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.2.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.3.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.3.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.3.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.3.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.3.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.3.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.3.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.3.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.3.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.4.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.4.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.4.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.4.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.4.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00006.safetensors'
INFO:hf-to-gguf:blk.10.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.10.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.10.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.10.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.10.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.10.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.10.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.10.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.10.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.11.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.11.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.11.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.11.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.4.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.4.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.4.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.4.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.5.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.5.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.5.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.5.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.5.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.5.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.5.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.5.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.5.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.6.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.6.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.6.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.6.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.6.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.6.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.6.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.6.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.6.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.7.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.7.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.7.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.7.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.7.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.7.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.7.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.7.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.7.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.8.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.8.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.8.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.8.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.8.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.8.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.8.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.8.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.8.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.9.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.9.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.9.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.9.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.9.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.9.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.9.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.9.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.9.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00006.safetensors'
INFO:hf-to-gguf:blk.11.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.11.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.11.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.11.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.11.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.12.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.12.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.12.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.12.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.12.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.12.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.12.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.12.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.12.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.13.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.13.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.13.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.13.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.13.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.13.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.13.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.13.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.13.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.14.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.14.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.14.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.14.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.14.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.14.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.14.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.14.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.14.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.15.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.15.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.15.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.15.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.15.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.15.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.15.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.15.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.15.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.16.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.16.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.16.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.16.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.16.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.16.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.16.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.16.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.16.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.17.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.17.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.17.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.17.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.17.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.17.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.17.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.17.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.17.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00006.safetensors'
INFO:hf-to-gguf:blk.18.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.18.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.18.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.18.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.18.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.18.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.18.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.18.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.18.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.19.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.19.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.19.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.19.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.19.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.19.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.19.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.19.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.19.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.20.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.20.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.20.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.20.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.20.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.20.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.20.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.20.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.20.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.21.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.21.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.21.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.21.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.21.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.21.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.21.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.21.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.21.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.22.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.22.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.22.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.22.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.22.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.22.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.22.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.22.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.22.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.23.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.23.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.23.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.23.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.23.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.23.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.23.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.23.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.23.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.24.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.24.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.24.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.24.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.24.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.24.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:gguf: loading model part 'model-00005-of-00006.safetensors'
INFO:hf-to-gguf:blk.24.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.24.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.24.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.25.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.25.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.25.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.25.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.25.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.25.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.25.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.25.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.25.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.26.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.26.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.26.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.26.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.26.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.26.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.26.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.26.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.26.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.27.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.27.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.27.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.27.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.27.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.27.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.27.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.27.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.27.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.28.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.28.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.28.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.28.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.28.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.28.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.28.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.28.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.28.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.29.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.29.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.29.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.29.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.29.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.29.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.29.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.29.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.29.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.30.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.30.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.30.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.30.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.30.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.30.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.30.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.30.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.30.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.31.ffn_gate.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.31.attn_k.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.31.attn_output.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.31.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.31.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:gguf: loading model part 'model-00006-of-00006.safetensors'
INFO:hf-to-gguf:output.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.31.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.31.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.31.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.31.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:output_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:Model successfully exported to 'Llama-3-Open-Ko-8B.gguf'
```

## gguf 양자화
```bash
snoopy_kr@Leeui-iMac Models % ../llama.cpp/build/bin/quantize Llama-3-Open-Ko-8B.gguf Llama-3-Open-Ko-8B-q5_k_m.gguf Q5_K_M
main: build = 2784 (a2ac89d6)
main: built with Apple clang version 15.0.0 (clang-1500.1.0.2.5) for x86_64-apple-darwin22.6.0
main: quantizing 'Llama-3-Open-Ko-8B.gguf' to 'Llama-3-Open-Ko-8B-q5_k_m.gguf' as Q5_K_M
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from Llama-3-Open-Ko-8B.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Llama-3-Open-Ko-8B
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336
llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  10:                          general.file_type u32              = 1
llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001
llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...
llama_model_loader: - type  f32:   65 tensors
llama_model_loader: - type  f16:  226 tensors
[   1/ 291]                    token_embd.weight - [ 4096, 128256,     1,     1], type =    f16, converting to q5_K .. size =  1002.00 MiB ->   344.44 MiB
[   2/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[   3/ 291]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB
[   4/ 291]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[   5/ 291]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[   6/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[   7/ 291]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[   8/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[   9/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  10/ 291]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  11/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  12/ 291]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB
[  13/ 291]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[  14/ 291]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[  15/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  16/ 291]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  17/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  18/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  19/ 291]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  20/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  21/ 291]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB
[  22/ 291]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[  23/ 291]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[  24/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  25/ 291]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  26/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  27/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  28/ 291]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  29/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  30/ 291]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB
[  31/ 291]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[  32/ 291]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[  33/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  34/ 291]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  35/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  36/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  37/ 291]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  38/ 291]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[  39/ 291]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  40/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  41/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  42/ 291]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  43/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  44/ 291]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[  45/ 291]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[  46/ 291]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[  47/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  48/ 291]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  49/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  50/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  51/ 291]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  52/ 291]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  53/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  54/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  55/ 291]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  56/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  57/ 291]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[  58/ 291]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[  59/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  60/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  61/ 291]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB
[  62/ 291]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[  63/ 291]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[  64/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  65/ 291]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  66/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  67/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  68/ 291]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  69/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  70/ 291]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[  71/ 291]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[  72/ 291]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[  73/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  74/ 291]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  75/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  76/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  77/ 291]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  78/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  79/ 291]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[  80/ 291]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[  81/ 291]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[  82/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  83/ 291]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  84/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  85/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  86/ 291]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[  87/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  88/ 291]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB
[  89/ 291]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[  90/ 291]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[  91/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  92/ 291]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  93/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  94/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[  95/ 291]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[  96/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[  97/ 291]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[  98/ 291]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[  99/ 291]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 100/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 101/ 291]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 102/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 103/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 104/ 291]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 105/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 106/ 291]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 107/ 291]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 108/ 291]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 109/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 110/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 111/ 291]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB
[ 112/ 291]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 113/ 291]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 114/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 115/ 291]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 116/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 117/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 118/ 291]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 119/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 120/ 291]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 121/ 291]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 122/ 291]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 123/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 124/ 291]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 125/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 126/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 127/ 291]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 128/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 129/ 291]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 130/ 291]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 131/ 291]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 132/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 133/ 291]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 134/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 135/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 136/ 291]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 137/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 138/ 291]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB
[ 139/ 291]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 140/ 291]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 141/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 142/ 291]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 143/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 144/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 145/ 291]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 146/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 147/ 291]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 148/ 291]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 149/ 291]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 150/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 151/ 291]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 152/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 153/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 154/ 291]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 155/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 156/ 291]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 157/ 291]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 158/ 291]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 159/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 160/ 291]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 161/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 162/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 163/ 291]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 164/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 165/ 291]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB
[ 166/ 291]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 167/ 291]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 168/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 169/ 291]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 170/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 171/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 172/ 291]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 173/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 174/ 291]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 175/ 291]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 176/ 291]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 177/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 178/ 291]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 179/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 180/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 181/ 291]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 182/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 183/ 291]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 184/ 291]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 185/ 291]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 186/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 187/ 291]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 188/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 189/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 190/ 291]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 191/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 192/ 291]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB
[ 193/ 291]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 194/ 291]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 195/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 196/ 291]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 197/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 198/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 199/ 291]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 200/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 201/ 291]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 202/ 291]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 203/ 291]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 204/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 205/ 291]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 206/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 207/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 208/ 291]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 209/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 210/ 291]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 211/ 291]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 212/ 291]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 213/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 214/ 291]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 215/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 216/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 217/ 291]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 218/ 291]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 219/ 291]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 220/ 291]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 221/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 222/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 223/ 291]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 224/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 225/ 291]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB
[ 226/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 227/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 228/ 291]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 229/ 291]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 230/ 291]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 231/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 232/ 291]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 233/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 234/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 235/ 291]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 236/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 237/ 291]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 238/ 291]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 239/ 291]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 240/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 241/ 291]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 242/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 243/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 244/ 291]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 245/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 246/ 291]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB
[ 247/ 291]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 248/ 291]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 249/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 250/ 291]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 251/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 252/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 253/ 291]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 254/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 255/ 291]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB
[ 256/ 291]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 257/ 291]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 258/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 259/ 291]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 260/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 261/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 262/ 291]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 263/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 264/ 291]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB
[ 265/ 291]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 266/ 291]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 267/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 268/ 291]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 269/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 270/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 271/ 291]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 272/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 273/ 291]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB
[ 274/ 291]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 275/ 291]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 276/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 277/ 291]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 278/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 279/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 280/ 291]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 281/ 291]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 282/ 291]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB
[ 283/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 284/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB
[ 285/ 291]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB
[ 286/ 291]                        output.weight - [ 4096, 128256,     1,     1], type =    f16, converting to q6_K .. size =  1002.00 MiB ->   410.98 MiB
[ 287/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 288/ 291]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB
[ 289/ 291]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 290/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
llama_model_quantize_internal: model size  = 15317.02 MB
llama_model_quantize_internal: quant size  =  5459.93 MB

main: quantize time = 102716.81 ms
main:    total time = 102716.81 ms
```

## [extra] cmake 설치
```bash
snoopy_kr@Leeui-iMac build % brew install cmake
Running `brew update --auto-update`...
==> Downloading https://ghcr.io/v2/homebrew/portable-ruby/portable-ruby/blobs/sha256:02180ca8b8295422ae84921bcf034b7ee8ce5575488bd5e6a37a192e53cd5d34
######################################################################### 100.0%
==> Pouring portable-ruby-3.1.4.el_capitan.bottle.tar.gz
==> Homebrew collects anonymous analytics.
Read the analytics documentation (and how to opt-out) here:
  https://docs.brew.sh/Analytics
No analytics have been recorded yet (nor will be during this `brew` run).

==> Homebrew is run entirely by unpaid volunteers. Please consider donating:
  https://github.com/Homebrew/brew#donations

Warning: Treating cmake as a formula. For the cask, use homebrew/cask/cmake or specify the `--cask` flag.
==> Downloading https://ghcr.io/v2/homebrew/core/cmake/manifests/3.29.2
######################################################################### 100.0%
==> Fetching cmake
==> Downloading https://ghcr.io/v2/homebrew/core/cmake/blobs/sha256:f3e0af8039fb
######################################################################### 100.0%
==> Pouring cmake--3.29.2.ventura.bottle.tar.gz
==> Caveats
To install the CMake documentation, run:
  brew install cmake-docs

Emacs Lisp files have been installed to:
  /usr/local/share/emacs/site-lisp/cmake
==> Summary
🍺  /usr/local/Cellar/cmake/3.29.2: 3,384 files, 59MB
==> Running `brew cleanup cmake`...
Disable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP.
Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).
```

## [extra] huggingface_hub 설치
```bash
snoopy_kr@Leeui-iMac llama.cpp % pip install huggingface_hub                
Requirement already satisfied: huggingface_hub in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (0.23.0)
Requirement already satisfied: filelock in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from huggingface_hub) (3.13.1)
Requirement already satisfied: fsspec>=2023.5.0 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from huggingface_hub) (2023.10.0)
Requirement already satisfied: packaging>=20.9 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from huggingface_hub) (23.1)
Requirement already satisfied: pyyaml>=5.1 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from huggingface_hub) (6.0.1)
Requirement already satisfied: requests in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from huggingface_hub) (2.31.0)
Requirement already satisfied: tqdm>=4.42.1 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from huggingface_hub) (4.65.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from huggingface_hub) (4.9.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from requests->huggingface_hub) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from requests->huggingface_hub) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from requests->huggingface_hub) (2.0.7)
Requirement already satisfied: certifi>=2017.4.17 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from requests->huggingface_hub) (2024.2.2)
```

## [extra] download.py 생성
```python
from huggingface_hub import snapshot_download
model_id="beomi/Llama-3-Open-Ko-8B"
snapshot_download(repo_id=model_id, local_dir="Llama-3-Open-Ko-8B",
                  local_dir_use_symlinks=False, revision="main")
```

## [extra] download model
```bash
snoopy_kr@Leeui-iMac test % python download.py 
/Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1194: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.
For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.
  warnings.warn(
generation_config.json: 100%|███████████████████| 132/132 [00:00<00:00, 751kB/s]
.gitattributes: 100%|██████████████████████| 1.57k/1.57k [00:00<00:00, 11.1MB/s]
config.json: 100%|█████████████████████████████| 698/698 [00:00<00:00, 4.48MB/s]
model.safetensors.index.json: 100%|████████| 23.9k/23.9k [00:00<00:00, 2.72MB/s]
README.md: 100%|███████████████████████████| 8.21k/8.21k [00:00<00:00, 46.9MB/s]
model-00004-of-00006.safetensors: 100%|████| 2.94G/2.94G [03:44<00:00, 13.1MB/s]
model-00002-of-00006.safetensors: 100%|████| 2.94G/2.94G [04:14<00:00, 11.5MB/s]
model-00006-of-00006.safetensors: 100%|████| 1.29G/1.29G [04:54<00:00, 4.36MB/s]
model-00003-of-00006.safetensors: 100%|████| 2.97G/2.97G [05:06<00:00, 9.67MB/s]
pytorch_model-00002-of-00006.bin: 100%|████| 2.94G/2.94G [06:01<00:00, 8.11MB/s]
pytorch_model.bin.index.json: 100%|████████| 23.9k/23.9k [00:00<00:00, 90.4MB/s]
special_tokens_map.json: 100%|█████████████████| 301/301 [00:00<00:00, 3.85MB/s]
tokenizer.json: 100%|██████████████████████| 9.09M/9.09M [00:03<00:00, 2.45MB/s]
tokenizer_config.json: 100%|███████████████| 51.0k/51.0k [00:00<00:00, 12.9MB/s]
pytorch_model-00006-of-00006.bin: 100%|████| 1.29G/1.29G [01:46<00:00, 12.0MB/s]
pytorch_model-00004-of-00006.bin: 100%|████| 2.94G/2.94G [03:11<00:00, 15.4MB/s]
pytorch_model-00003-of-00006.bin: 100%|████| 2.97G/2.97G [03:45<00:00, 13.2MB/s]
model-00005-of-00006.safetensors: 100%|████| 2.94G/2.94G [08:04<00:00, 6.06MB/s]
pytorch_model-00001-of-00006.bin: 100%|████| 3.00G/3.00G [08:37<00:00, 5.79MB/s]
model-00001-of-00006.safetensors: 100%|████| 3.00G/3.00G [08:46<00:00, 5.69MB/s]
pytorch_model-00005-of-00006.bin: 100%|████| 2.94G/2.94G [03:53<00:00, 12.6MB/s]
Fetching 21 files: 100%|████████████████████████| 21/21 [08:52<00:00, 25.35s/it]
```

## [extra] llama.cpp requirements 설치
```bash
snoopy_kr@Leeui-iMac Working % pip install -r llama.cpp/requirements.txt
Requirement already satisfied: numpy~=1.24.4 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from -r llama.cpp/./requirements/requirements-convert.txt (line 1)) (1.24.4)
Requirement already satisfied: sentencepiece~=0.1.98 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from -r llama.cpp/./requirements/requirements-convert.txt (line 2)) (0.1.99)
Requirement already satisfied: transformers<5.0.0,>=4.35.2 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from -r llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.40.1)
Requirement already satisfied: gguf>=0.1.0 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from -r llama.cpp/./requirements/requirements-convert.txt (line 4)) (0.6.0)
Requirement already satisfied: protobuf<5.0.0,>=4.21.0 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from -r llama.cpp/./requirements/requirements-convert.txt (line 5)) (4.25.3)
Requirement already satisfied: torch~=2.1.1 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from -r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.2)
Requirement already satisfied: einops~=0.7.0 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from -r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 3)) (0.7.0)
Requirement already satisfied: filelock in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.13.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.23.0)
Requirement already satisfied: packaging>=20.0 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (23.1)
Requirement already satisfied: pyyaml>=5.1 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (6.0.1)
Requirement already satisfied: regex!=2019.12.17 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2023.10.3)
Requirement already satisfied: requests in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.31.0)
Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.19.1)
Requirement already satisfied: safetensors>=0.4.1 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.4.3)
Requirement already satisfied: tqdm>=4.27 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.65.0)
Requirement already satisfied: typing-extensions in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (4.9.0)
Requirement already satisfied: sympy in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.12)
Requirement already satisfied: networkx in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.1)
Requirement already satisfied: jinja2 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.1.3)
Requirement already satisfied: fsspec in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2023.10.0)
Requirement already satisfied: MarkupSafe>=2.0 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from jinja2->torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.3)
Requirement already satisfied: charset-normalizer<4,>=2 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.0.7)
Requirement already satisfied: certifi>=2017.4.17 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2024.2.2)
Requirement already satisfied: mpmath>=0.19 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from sympy->torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.3.0)
```

## [reference] convert.py help
```bash
snoopy_kr@Leeui-iMac Working % python llama.cpp/convert.py -h         
usage: convert.py [-h] [--dump] [--dump-single] [--vocab-only] [--no-vocab]
                  [--outtype {f32,f16,q8_0}] [--vocab-dir VOCAB_DIR]
                  [--vocab-type VOCAB_TYPE] [--outfile OUTFILE] [--ctx CTX]
                  [--concurrency CONCURRENCY] [--big-endian] [--pad-vocab]
                  [--skip-unknown] [--verbose]
                  model

Convert a LLaMA model to a GGML compatible file

positional arguments:
  model                 directory containing model file, or model file itself
                        (*.pth, *.pt, *.bin)

options:
  -h, --help            show this help message and exit
  --dump                don't convert, just show what's in the model
  --dump-single         don't convert, just show what's in a single model file
  --vocab-only          extract only the vocab
  --no-vocab            store model without the vocab
  --outtype {f32,f16,q8_0}
                        output format - note: q8_0 may be very slow (default:
                        f16 or f32 based on input)
  --vocab-dir VOCAB_DIR
                        directory containing tokenizer.model, if separate from
                        model file
  --vocab-type VOCAB_TYPE
                        vocab types to try in order, choose from 'spm', 'bpe',
                        'hfft' (default: spm,hfft)
  --outfile OUTFILE     path to write to; default: based on input
  --ctx CTX             model training context (default: based on input)
  --concurrency CONCURRENCY
                        concurrency used for conversion (default: 8)
  --big-endian          model is executed on big endian machine
  --pad-vocab           add pad tokens when model vocab expects more than
                        tokenizer metadata provides
  --skip-unknown        skip unknown tensor names instead of failing
  --verbose             increase output verbosity
```  

