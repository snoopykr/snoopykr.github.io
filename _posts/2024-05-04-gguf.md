---
layout: single
title: "gguf íŒŒì¼ ë§Œë“¤ê¸° ë”ì´ìƒ í—¤ë©”ì§€ ë§ìž."
categories : AI
tag: [huggingface, llama3, gguf, cmake]
toc: true
---

gguf íŒŒì¼ì„ ìƒì„±í•˜ê¸° ìœ„í•œ ê°€ì´ë“œ

## ì°¸ì¡°
huggingface
- [https://huggingface.co](https://huggingface.co){:target="_blank"}

llama.cpp
- [https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp){:target="_blank"}

k-quants
- [https://github.com/ggerganov/llama.cpp/pull/1684](https://github.com/ggerganov/llama.cpp/pull/1684){:target="_blank"}

Tutorial: How to convert HuggingFace model to GGUF format #2948
- [https://github.com/ggerganov/llama.cpp/discussions/2948](https://github.com/ggerganov/llama.cpp/discussions/2948){:target="_blank"}

Refactor convert.py and add support for Metas official Llama 3 model #6819
- [https://github.com/ggerganov/llama.cpp/issues/6819](https://github.com/ggerganov/llama.cpp/issues/6819){:target="_blank"}

## llama.cpp ì¤€ë¹„
```bash
snoopy_kr@Leeui-iMac Working % git clone https://github.com/ggerganov/llama.cpp
Cloning into 'llama.cpp'...
remote: Enumerating objects: 23768, done.
remote: Counting objects: 100% (127/127), done.
remote: Compressing objects: 100% (87/87), done.
remote: Total 23768 (delta 59), reused 90 (delta 40), pack-reused 23641
Receiving objects: 100% (23768/23768), 38.14 MiB | 21.42 MiB/s, done.
Resolving deltas: 100% (16793/16793), done.
```

```bash
snoopy_kr@Leeui-iMac Working % cd llama.cpp
```

```bash
snoopy_kr@Leeui-iMac llama.cpp % mkdir build

```

```bash
snoopy_kr@Leeui-iMac llama.cpp % cd build
```

[extra] cmake ì„¤ì¹˜

```bash
snoopy_kr@Leeui-iMac build % cmake ..
-- The C compiler identification is AppleClang 15.0.0.15000100
-- The CXX compiler identification is AppleClang 15.0.0.15000100
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/local/bin/git (found version "2.43.0")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- Accelerate framework found
-- Metal framework found
-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with LLAMA_CCACHE=OFF
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- x86 detected
-- Configuring done (5.6s)
-- Generating done (0.4s)
-- Build files have been written to: /Users/snoopy_kr/Working/llama.cpp/build
```

```bash
snoopy_kr@Leeui-iMac build % cmake --build . --config Release
[  0%] Compiling Metal kernels
[  0%] Built target ggml-metal
[  0%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  1%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  2%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  2%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml-metal.m.o
[  4%] Building CXX object CMakeFiles/ggml.dir/sgemm.cpp.o
[  4%] Built target ggml
[  5%] Linking CXX static library libggml_static.a
[  5%] Built target ggml_static

[ === ìƒëžµ === ]

[ 96%] Built target server
[ 97%] Building CXX object examples/export-lora/CMakeFiles/export-lora.dir/export-lora.cpp.o
[ 97%] Linking CXX executable ../../bin/export-lora
[ 97%] Built target export-lora
[ 98%] Building CXX object pocs/vdot/CMakeFiles/vdot.dir/vdot.cpp.o
[ 99%] Linking CXX executable ../../bin/vdot
[ 99%] Built target vdot
[100%] Building CXX object pocs/vdot/CMakeFiles/q8dot.dir/q8dot.cpp.o
[100%] Linking CXX executable ../../bin/q8dot
[100%] Built target q8dot
```

[extra] huggingface_hub ì„¤ì¹˜

[extra] download.py ìž‘ì—…

[extra] download model

[extra] llama.cpp requirements ì„¤ì¹˜

## Modelì„ ggufë¡œ ë³€ê²½
```bash
snoopy_kr@Leeui-iMac Models % python ../llama.cpp/convert-hf-to-gguf.py Llama-3-Open-Ko-8B --outfile Llama-3-Open-Ko-8B.gguf
INFO:hf-to-gguf:Loading model: Llama-3-Open-Ko-8B
INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only
INFO:hf-to-gguf:Set model parameters
INFO:hf-to-gguf:gguf: context length = 8192
INFO:hf-to-gguf:gguf: embedding length = 4096
INFO:hf-to-gguf:gguf: feed forward length = 14336
INFO:hf-to-gguf:gguf: head count = 32
INFO:hf-to-gguf:gguf: key-value head count = 8
INFO:hf-to-gguf:gguf: rope theta = 500000.0
INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05

[ === ìƒëžµ === ]

INFO:hf-to-gguf:blk.31.attn_q.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.31.attn_v.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:gguf: loading model part 'model-00006-of-00006.safetensors'
INFO:hf-to-gguf:output.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.31.attn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:blk.31.ffn_down.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.31.ffn_up.weight, n_dims = 2, torch.bfloat16 --> float16
INFO:hf-to-gguf:blk.31.ffn_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:output_norm.weight, n_dims = 1, torch.bfloat16 --> float32
INFO:hf-to-gguf:Model successfully exported to 'Llama-3-Open-Ko-8B.gguf'
```

## gguf ì–‘ìží™”
```bash
snoopy_kr@Leeui-iMac Models % ../llama.cpp/build/bin/quantize Llama-3-Open-Ko-8B.gguf Llama-3-Open-Ko-8B-q5_k_m.gguf Q5_K_M
main: build = 2784 (a2ac89d6)
main: built with Apple clang version 15.0.0 (clang-1500.1.0.2.5) for x86_64-apple-darwin22.6.0
main: quantizing 'Llama-3-Open-Ko-8B.gguf' to 'Llama-3-Open-Ko-8B-q5_k_m.gguf' as Q5_K_M
llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from Llama-3-Open-Ko-8B.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.name str              = Llama-3-Open-Ko-8B
llama_model_loader: - kv   2:                          llama.block_count u32              = 32
llama_model_loader: - kv   3:                       llama.context_length u32              = 8192
llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096
llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336

[ === ìƒëžµ === ]

[ 286/ 291]                        output.weight - [ 4096, 128256,     1,     1], type =    f16, converting to q6_K .. size =  1002.00 MiB ->   410.98 MiB
[ 287/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 288/ 291]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB
[ 289/ 291]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB
[ 290/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
[ 291/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB
llama_model_quantize_internal: model size  = 15317.02 MB
llama_model_quantize_internal: quant size  =  5459.93 MB

main: quantize time = 102716.81 ms
main:    total time = 102716.81 ms
```

## [extra] cmake ì„¤ì¹˜
```bash
snoopy_kr@Leeui-iMac build % brew install cmake
Running `brew update --auto-update`...
==> Downloading https://ghcr.io/v2/homebrew/portable-ruby/portable-ruby/blobs/sha256:02180ca8b8295422ae84921bcf034b7ee8ce5575488bd5e6a37a192e53cd5d34
######################################################################### 100.0%
==> Pouring portable-ruby-3.1.4.el_capitan.bottle.tar.gz
==> Homebrew collects anonymous analytics.
Read the analytics documentation (and how to opt-out) here:
  https://docs.brew.sh/Analytics
No analytics have been recorded yet (nor will be during this `brew` run).

==> Homebrew is run entirely by unpaid volunteers. Please consider donating:
  https://github.com/Homebrew/brew#donations

Warning: Treating cmake as a formula. For the cask, use homebrew/cask/cmake or specify the `--cask` flag.
==> Downloading https://ghcr.io/v2/homebrew/core/cmake/manifests/3.29.2
######################################################################### 100.0%
==> Fetching cmake
==> Downloading https://ghcr.io/v2/homebrew/core/cmake/blobs/sha256:f3e0af8039fb
######################################################################### 100.0%
==> Pouring cmake--3.29.2.ventura.bottle.tar.gz
==> Caveats
To install the CMake documentation, run:
  brew install cmake-docs

Emacs Lisp files have been installed to:
  /usr/local/share/emacs/site-lisp/cmake
==> Summary
ðŸº  /usr/local/Cellar/cmake/3.29.2: 3,384 files, 59MB
==> Running `brew cleanup cmake`...
Disable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP.
Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).
```

## [extra] huggingface_hub ì„¤ì¹˜
```bash
snoopy_kr@Leeui-iMac llama.cpp % pip install huggingface_hub                
Requirement already satisfied: huggingface_hub in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (0.23.0)
Requirement already satisfied: filelock in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from huggingface_hub) (3.13.1)
Requirement already satisfied: fsspec>=2023.5.0 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from huggingface_hub) (2023.10.0)
Requirement already satisfied: packaging>=20.9 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from huggingface_hub) (23.1)
Requirement already satisfied: pyyaml>=5.1 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from huggingface_hub) (6.0.1)
Requirement already satisfied: requests in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from huggingface_hub) (2.31.0)
Requirement already satisfied: tqdm>=4.42.1 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from huggingface_hub) (4.65.0)
Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from huggingface_hub) (4.9.0)
Requirement already satisfied: charset-normalizer<4,>=2 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from requests->huggingface_hub) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from requests->huggingface_hub) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from requests->huggingface_hub) (2.0.7)
Requirement already satisfied: certifi>=2017.4.17 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from requests->huggingface_hub) (2024.2.2)
```

## [extra] download.py ìž‘ì—…
```python
from huggingface_hub import snapshot_download
model_id="beomi/Llama-3-Open-Ko-8B"
snapshot_download(repo_id=model_id, local_dir="Llama-3-Open-Ko-8B",
                  local_dir_use_symlinks=False, revision="main")
```

## [extra] download model
```bash
snoopy_kr@Leeui-iMac test % python download.py 
/Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1194: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.
For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.
  warnings.warn(
generation_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 132/132 [00:00<00:00, 751kB/s]
.gitattributes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.57k/1.57k [00:00<00:00, 11.1MB/s]
config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 698/698 [00:00<00:00, 4.48MB/s]
model.safetensors.index.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23.9k/23.9k [00:00<00:00, 2.72MB/s]
README.md: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8.21k/8.21k [00:00<00:00, 46.9MB/s]
model-00004-of-00006.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 2.94G/2.94G [03:44<00:00, 13.1MB/s]
model-00002-of-00006.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 2.94G/2.94G [04:14<00:00, 11.5MB/s]
model-00006-of-00006.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 1.29G/1.29G [04:54<00:00, 4.36MB/s]
model-00003-of-00006.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 2.97G/2.97G [05:06<00:00, 9.67MB/s]
pytorch_model-00002-of-00006.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 2.94G/2.94G [06:01<00:00, 8.11MB/s]
pytorch_model.bin.index.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23.9k/23.9k [00:00<00:00, 90.4MB/s]
special_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 301/301 [00:00<00:00, 3.85MB/s]
tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.09M/9.09M [00:03<00:00, 2.45MB/s]
tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51.0k/51.0k [00:00<00:00, 12.9MB/s]
pytorch_model-00006-of-00006.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 1.29G/1.29G [01:46<00:00, 12.0MB/s]
pytorch_model-00004-of-00006.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 2.94G/2.94G [03:11<00:00, 15.4MB/s]
pytorch_model-00003-of-00006.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 2.97G/2.97G [03:45<00:00, 13.2MB/s]
model-00005-of-00006.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 2.94G/2.94G [08:04<00:00, 6.06MB/s]
pytorch_model-00001-of-00006.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 3.00G/3.00G [08:37<00:00, 5.79MB/s]
model-00001-of-00006.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 3.00G/3.00G [08:46<00:00, 5.69MB/s]
pytorch_model-00005-of-00006.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 2.94G/2.94G [03:53<00:00, 12.6MB/s]
Fetching 21 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [08:52<00:00, 25.35s/it]
```

## [extra] llama.cpp requirements ì„¤ì¹˜
```bash
snoopy_kr@Leeui-iMac Working % pip install -r llama.cpp/requirements.txt
Requirement already satisfied: numpy~=1.24.4 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from -r llama.cpp/./requirements/requirements-convert.txt (line 1)) (1.24.4)
Requirement already satisfied: sentencepiece~=0.1.98 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from -r llama.cpp/./requirements/requirements-convert.txt (line 2)) (0.1.99)
Requirement already satisfied: transformers<5.0.0,>=4.35.2 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from -r llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.40.1)
Requirement already satisfied: gguf>=0.1.0 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from -r llama.cpp/./requirements/requirements-convert.txt (line 4)) (0.6.0)
Requirement already satisfied: protobuf<5.0.0,>=4.21.0 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from -r llama.cpp/./requirements/requirements-convert.txt (line 5)) (4.25.3)
Requirement already satisfied: torch~=2.1.1 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from -r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.2)
Requirement already satisfied: einops~=0.7.0 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from -r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 3)) (0.7.0)
Requirement already satisfied: filelock in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.13.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.23.0)
Requirement already satisfied: packaging>=20.0 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (23.1)
Requirement already satisfied: pyyaml>=5.1 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (6.0.1)
Requirement already satisfied: regex!=2019.12.17 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2023.10.3)
Requirement already satisfied: requests in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.31.0)
Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.19.1)
Requirement already satisfied: safetensors>=0.4.1 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.4.3)
Requirement already satisfied: tqdm>=4.27 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.65.0)
Requirement already satisfied: typing-extensions in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (4.9.0)
Requirement already satisfied: sympy in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.12)
Requirement already satisfied: networkx in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.1)
Requirement already satisfied: jinja2 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.1.3)
Requirement already satisfied: fsspec in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2023.10.0)
Requirement already satisfied: MarkupSafe>=2.0 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from jinja2->torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.3)
Requirement already satisfied: charset-normalizer<4,>=2 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.0.7)
Requirement already satisfied: certifi>=2017.4.17 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2024.2.2)
Requirement already satisfied: mpmath>=0.19 in /Users/snoopy_kr/Apps/anaconda3/lib/python3.11/site-packages (from sympy->torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.3.0)
```

## [reference] convert.py help
```bash
snoopy_kr@Leeui-iMac Working % python llama.cpp/convert.py -h         
usage: convert.py [-h] [--dump] [--dump-single] [--vocab-only] [--no-vocab]
                  [--outtype {f32,f16,q8_0}] [--vocab-dir VOCAB_DIR]
                  [--vocab-type VOCAB_TYPE] [--outfile OUTFILE] [--ctx CTX]
                  [--concurrency CONCURRENCY] [--big-endian] [--pad-vocab]
                  [--skip-unknown] [--verbose]
                  model

Convert a LLaMA model to a GGML compatible file

positional arguments:
  model                 directory containing model file, or model file itself
                        (*.pth, *.pt, *.bin)

options:
  -h, --help            show this help message and exit
  --dump                don't convert, just show what's in the model
  --dump-single         don't convert, just show what's in a single model file
  --vocab-only          extract only the vocab
  --no-vocab            store model without the vocab
  --outtype {f32,f16,q8_0}
                        output format - note: q8_0 may be very slow (default:
                        f16 or f32 based on input)
  --vocab-dir VOCAB_DIR
                        directory containing tokenizer.model, if separate from
                        model file
  --vocab-type VOCAB_TYPE
                        vocab types to try in order, choose from 'spm', 'bpe',
                        'hfft' (default: spm,hfft)
  --outfile OUTFILE     path to write to; default: based on input
  --ctx CTX             model training context (default: based on input)
  --concurrency CONCURRENCY
                        concurrency used for conversion (default: 8)
  --big-endian          model is executed on big endian machine
  --pad-vocab           add pad tokens when model vocab expects more than
                        tokenizer metadata provides
  --skip-unknown        skip unknown tensor names instead of failing
  --verbose             increase output verbosity
```  

